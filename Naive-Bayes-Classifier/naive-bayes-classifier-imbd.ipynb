{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier Simulation on IMDB Dataset\n",
    "\n",
    "Name: Stanley Nathanael Wijaya\n",
    "<br>\n",
    "NIM: 2702217125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \n",
    "    stopwords = set([\"is\", \"it\", \"this\", \"the\", \"i\", \"so\", \"be\", \"to\", \"and\", \"a\", \"they\", \"may\", \"in\", \"on\", \"for\", \"ever\"])  \n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "\n",
    "    bigrams = [\" \".join(words[i:i+2]) for i in range(len(words)-1)]\n",
    "    return words + bigrams \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    (\"-\", \"It is Boring and it is a bad super hero movie\"),\n",
    "    (\"+\", \"The best movie i have ever seen so far in 2022\"),\n",
    "    (\"+\", \"This may be the best batman movie ever\"),\n",
    "    (\"+\", \"They are pretty good\"),\n",
    "    (\"+\", \"The prop and tech design is so good and fun\"),\n",
    "    (\"-\", \"May be the least entertaining batman movie\"),\n",
    "    (\"+\", \"Really enjoyed this movie\"),\n",
    "    (\"-\", \"The batman proves to be bad and violent movie\"),\n",
    "    (\"-\", \"So boring and bleak and cynical\")\n",
    "]\n",
    "\n",
    "test_data = [\n",
    "    (\"?\", \"Boring and violent\"),\n",
    "    (\"?\", \"Enjoyed just watched this movie\"),\n",
    "    (\"?\", \"The first half is good but i hate the ending\"),\n",
    "    (\"?\", \"Pretty good and fun movie\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = []\n",
    "train_labels = []\n",
    "\n",
    "for label, text in training_data:\n",
    "    train_texts.append(preprocess_text(text))\n",
    "    train_labels.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, smoothing=1):\n",
    "        self.smoothing = smoothing\n",
    "        self.vocab = set()\n",
    "        self.word_counts = {\"+\": defaultdict(int), \"-\": defaultdict(int)}\n",
    "        self.class_counts = {\"+\": 0, \"-\": 0}\n",
    "        \n",
    "        # Kata negatif kuat dengan bobot tinggi\n",
    "        self.negative_boost = {\"hate\", \"boring\", \"bad\", \"worst\", \"cynical\", \"bleak\", \"violent\"}  \n",
    "        \n",
    "        # Kata kontradiktif yang harus diperhatikan\n",
    "        self.contradictory_words = {\"but\", \"however\", \"although\", \"though\"}\n",
    "\n",
    "    def train(self, texts, labels):\n",
    "        \n",
    "        for words, label in zip(texts, labels):\n",
    "            self.class_counts[label] += 1\n",
    "            for i, word in enumerate(words):\n",
    "                # Jika kata adalah kata negatif kuat, tambahkan bobot lebih besar\n",
    "                if word in self.negative_boost:\n",
    "                    self.word_counts[label][word] += 5  # Tambahkan bobot 5x\n",
    "                else:\n",
    "                    self.word_counts[label][word] += 1\n",
    "                self.vocab.add(word)\n",
    "                \n",
    "                # Jika kata adalah kata kontradiktif, beri penalti jika setelahnya ada kata negatif\n",
    "                if word in self.contradictory_words and i < len(words) - 1:\n",
    "                    next_word = words[i + 1]\n",
    "                    if next_word in self.negative_boost:\n",
    "                        self.word_counts[\"-\"][next_word] += 5  # Tambahkan penalti lebih besar\n",
    "\n",
    "        self.total_words = {\"+\": sum(self.word_counts[\"+\"].values()), \"-\": sum(self.word_counts[\"-\"].values())}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    def predict(self, text):\n",
    "        \n",
    "        words = preprocess_text(text)\n",
    "        \n",
    "        # Prior Probabilities\n",
    "        total_samples = sum(self.class_counts.values())\n",
    "        log_prob_pos = np.log(self.class_counts[\"+\"] / total_samples)\n",
    "        log_prob_neg = np.log(self.class_counts[\"-\"] / total_samples)\n",
    "\n",
    "        # Likelihood Calculation with Laplace Smoothing\n",
    "        for word in words:\n",
    "            pos_word_prob = (self.word_counts[\"+\"][word] + self.smoothing) / (self.total_words[\"+\"] + self.smoothing * self.vocab_size)\n",
    "            neg_word_prob = (self.word_counts[\"-\"][word] + self.smoothing) / (self.total_words[\"-\"] + self.smoothing * self.vocab_size)\n",
    "\n",
    "            log_prob_pos += np.log(pos_word_prob)\n",
    "            log_prob_neg += np.log(neg_word_prob)\n",
    "\n",
    "        return \"+\" if log_prob_pos > log_prob_neg else \"-\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier()\n",
    "classifier.train(train_texts, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis:\n",
      "'Boring and violent' -> -\n",
      "'Enjoyed just watched this movie' -> +\n",
      "'The first half is good but i hate the ending' -> +\n",
      "'Pretty good and fun movie' -> +\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentiment Analysis:\")\n",
    "for label, text in test_data:\n",
    "    prediction = classifier.predict(text)\n",
    "    print(f\"'{text}' -> {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the Sentiment Analysis Result:\n",
    "<ul>\n",
    "    <li>'Boring and violent' -> - (True Negative)</li>\n",
    "    <li>'Enjoyed just watched this movie' -> + (True Positive)</li>\n",
    "    <li>'The first half is good but i hate the ending' -> + (It depends, however maybe it's False Positive)</li>\n",
    "    <li>'Pretty good and fun movie' -> + (True Positive)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>Striving for Excellence üî•‚ù§Ô∏è‚Äçüî•</code>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
