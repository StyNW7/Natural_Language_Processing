{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8fd13aa",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aefaea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the libraries needed\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.classify import NaiveBayesClassifier, accuracy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "import spacy\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc2745a",
   "metadata": {},
   "source": [
    "# Read Data + Settings Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7f1b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting variables\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Read CSV\n",
    "dataset = pd.read_csv(\"Dataset/imdb-movies-dataset.csv\")\n",
    "\n",
    "dataset.isnull().sum() # Cek apakah ada data yang kosong / ga lengkap\n",
    "dataset = dataset.dropna() #  Drop data yang tidak lengkap / null\n",
    "\n",
    "# Tambahin kolom Sentiment karna belom ada sesuai ketentuan\n",
    "dataset['Sentiment'] = dataset['Rating'].apply(lambda x: 'positive' if x > 5 else 'negative') \n",
    "dataset.head(5)\n",
    "\n",
    "# dataset['Sentiment'].value_counts() # Jumlah berapa yang positive dan negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9260dfe",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1017a8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sentence):\n",
    "    # Tokenize\n",
    "    word_list = word_tokenize(sentence)\n",
    "    word_list = [word.lower() for word in word_list] # lower biar aman\n",
    "    \n",
    "    # Stopwords\n",
    "    no_stopwords = [token for token in word_list if token not in eng_stopwords]\n",
    "    \n",
    "    # Remove Punctuation\n",
    "    no_punc = [token for token in no_stopwords if token.isalpha()] \n",
    "    \n",
    "    # Stemming\n",
    "    stemmed = [stemmer.stem(token) for token in no_punc]\n",
    "    \n",
    "    # Lemmatizing\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in stemmed] # langsung pake default, kalo mau lebih bagus bikin function get_tag kaya pas quiz sebelumnya\n",
    "    \n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be112355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Distribution\n",
    "X = dataset[\"Review\"]\n",
    "Y = dataset[\"Sentiment\"]\n",
    "\n",
    "all_reviews = ' '.join(X)\n",
    "all_tokens = preprocess_text(all_reviews)\n",
    "\n",
    "freq_dist = FreqDist(all_tokens)\n",
    "\n",
    "print(freq_dist.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32dd2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Text Features\n",
    "def extract_feature(review):\n",
    "    features = {} # simpan dalam bentuk dictionary\n",
    "    for word in freq_dist.keys():\n",
    "        features[word] = (word in review)\n",
    "    \n",
    "    return features\n",
    "\n",
    "feature_sets = [(extract_feature(preprocess_text(review)), sentiment) for (review, sentiment) in zip(X,Y)]\n",
    "\n",
    "# Opsional kalo disuruh aja\n",
    "from random import shuffle\n",
    "shuffle(feature_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30713f92",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd0fe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load & Train Model\n",
    "def train_and_save_model():\n",
    "    train_count = int(len(feature_sets) * 0.8) # Sesuain ama ketentuan soal ya dimintanya berapa persentase testing / training nya\n",
    "    train_set = feature_sets[:train_count]\n",
    "    test_set = feature_sets[train_count:]\n",
    "\n",
    "    classifier = NaiveBayesClassifier.train(train_set)\n",
    "    test_accuracy = accuracy(classifier, test_set)\n",
    "    print(f\"Akurasi data test: {test_accuracy * 100:.2f}%\")\n",
    "    classifier.show_most_informative_features(5)\n",
    "\n",
    "    # Buat read / write file juga bebas mau pake cara ini atau pake yg syntax with open (\"file_name\", \"action\") as file:\n",
    "    file = open(\"model.pickle\", \"wb\") # Kalo pake cara ini jangan lupa file.close()\n",
    "    pickle.dump(classifier, file)\n",
    "    file.close()\n",
    "\n",
    "    return classifier\n",
    "        \n",
    "def load_model():\n",
    "    if os.path.exists(\"./model.pickle\"): # Validasi kaya gini bebas mau pake os atau pake try except\n",
    "        file = open(\"model.pickle\", \"rb\") \n",
    "        classifier = pickle.load(file)\n",
    "        print(\"Model loaded successfully.\")\n",
    "        file.close()\n",
    "    else:\n",
    "        print(\"Model not found! Training model...\")\n",
    "        classifier = train_and_save_model()\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80dad09",
   "metadata": {},
   "source": [
    "### Embedding Language Model\n",
    "(Harusnya cuma salah satu aja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b194d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "def tf_idf(query):\n",
    "    vectorizer = TfidfVectorizer(stop_words= 'english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(dataset[\"Review\"]) \n",
    "    # hitung .fit_transform buat hitung frekuensi kemunculan kata + ubah jadi vektor\n",
    "\n",
    "    query_vec = vectorizer.transform([query]) # ubah query / inputted sentence jadi vektor. Pake [] karena nerimanya dalam bentuk list\n",
    "\n",
    "    similarity = cosine_similarity(query_vec, tfidf_matrix).flatten() # Ubah hasil jadi 1 Dimensi \n",
    "    dataset['Similarity'] = similarity # Tambahin kolom baru di dataframe kita dengan judul \"Similarity\"\n",
    "\n",
    "    dataset_sorted = dataset.sort_values(by='Similarity', ascending= False) # Sort dari hasil nilai Similarity Besar -> Kecil\n",
    "    \n",
    "    print(\"\\nTop 2 Movie Recommendation for you:\")\n",
    "    print(\"1: \", dataset_sorted.iloc[0,0]) \n",
    "    print(\"2: \", dataset_sorted.iloc[1,0])\n",
    "     # iloc = \"integer location\". Digunain untuk ambil data per row, col. Jadi iloc[0,0] ambil baris pertama dari data dan kolom 0 yang berupa titlenya. Kalau mau ambil semua data dalam 1 row berarti iloc[0] aja\n",
    "    \n",
    "# Ngram-Models (Unigram, Bigram, Trigram)\n",
    "def ngram(query):\n",
    "    ngram_n = 3 \n",
    "    ngram_range = (1, ngram_n)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(ngram_range= ngram_range, stop_words='english') # bekerja dari unigram - trigram\n",
    "    tfidf_matrix = vectorizer.fit_transform(dataset[\"Review\"])\n",
    "\n",
    "    query_vec = vectorizer.transform([query])\n",
    "\n",
    "    similarity = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "    dataset['Similarity'] = similarity\n",
    "\n",
    "    dataset_sorted = dataset.sort_values(by='Similarity', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 2 Movie Recommendation for you:\")\n",
    "    print(\"1: \", dataset_sorted.iloc[0,0])\n",
    "    print(\"2: \", dataset_sorted.iloc[1,0])\n",
    "    \n",
    "# Word2Vec\n",
    "# Word2Vec: Menghasilkan vektor level kata. Diperlukan fungsi tambahan (seperti avg_vector) untuk hasilin vector query / sentence supaya bisa dibandingin pake consine_similarity\n",
    "def avg_vector(tokens, model): # Intinya function buat hasilin vektor rata\" dari sebuah query / sentence input\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if not vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis= 0)  \n",
    "\n",
    "def word2vec(query):\n",
    "    tokenized_docs = [preprocess_text(token) for token in dataset[\"Review\"]]\n",
    "    w2v_model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, sg = 1) # ini pake skip-gram kalo sg = 1, sg = 0 pake CBOW\n",
    "\n",
    "    query_vec = avg_vector(preprocess_text(query), w2v_model).reshape(1, -1) # Di reshape untuk ikutin ketentuan ketika digunakan di cosine_similarity\n",
    "\n",
    "    review_vec = [avg_vector(preprocess_text(token), w2v_model) for token in dataset[\"Review\"]]\n",
    "    similarity = cosine_similarity(query_vec, review_vec)[0]\n",
    "\n",
    "    dataset['Similarity'] = similarity\n",
    "\n",
    "    dataset_sorted = dataset.sort_values(by='Similarity', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 2 Movie Recommendation for you:\")\n",
    "    print(\"1: \", dataset_sorted.iloc[0,0])\n",
    "    print(\"2: \", dataset_sorted.iloc[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f62029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER with Spacy\n",
    "paragraph = ' '.join(dataset['Review'].head(500))\n",
    "\n",
    "# spacy.cli.download('en_core_web_sm')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# nlp.max_length = 10000000   \n",
    "doc = nlp(paragraph)\n",
    "\n",
    "categories = {}\n",
    "for ent in doc.ents:\n",
    "    label = ent.label_\n",
    "    if label not in categories:\n",
    "        categories[label] = []\n",
    "    categories[label].append(ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad67660",
   "metadata": {},
   "source": [
    "- nlp.max_length = 10000000: digunain kalo misal mau lebih dari batas default yaitu 1.000.000. Misalnya kalo banyak banget jumlah katanya yang di dataset. Tapi gausah pake gapapa tapi solusinya di limit aja jadi 500 baris pertama yang diproses biar ga keberatan ðŸ˜…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aaeb2d",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f10decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_review = \"No Review\"\n",
    "my_category = \"Unknown\"\n",
    "\n",
    "def menu_1(loaded_classifier):\n",
    "    global my_review, my_category\n",
    "    query = input(\"My query: \")\n",
    "    word_list = word_tokenize(query)\n",
    "    \n",
    "    if len(word_list) > 20: # Validasinya nanti sesuain aja ya ama soal UAP nya, ini ngecek apakah lebih dari 20 kata ato engga\n",
    "        my_review = query\n",
    "        query_processed = preprocess_text(query)\n",
    "        input_feature = extract_feature(query_processed)\n",
    "        my_category = loaded_classifier.classify(input_feature)\n",
    "        print(\"Review successfully updated!\")\n",
    "    else:\n",
    "        print(f\"Input must be more than 20 words. Your input has {len(word_list)} words.\")\n",
    "\n",
    "def menu_2(): \n",
    "    if my_review == \"No Review\":\n",
    "        print(\"Please Input Review First\")\n",
    "    else:\n",
    "        print(\"\\nChoose Language Model\")\n",
    "        print(\"1. Word2Vec \\n2. TF-IDF \\n3. N-GRAM\")\n",
    "        x = input(\"Chosen model: \")\n",
    "        \n",
    "        if x == '1':\n",
    "            ngram(my_review)\n",
    "        elif x == '2':\n",
    "            tf_idf(my_review)\n",
    "        elif x == '3':\n",
    "            word2vec(my_review)\n",
    "        else:\n",
    "            print(\"Invalid input\")\n",
    "\n",
    "def menu_3():\n",
    "    print(\"\\nNamed Entity Recognition\")\n",
    "    for label, ent in categories.items():\n",
    "        print(f'{label}: {\", \".join(ent)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdca220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_menu():\n",
    "    loaded_classifier = load_model()\n",
    "\n",
    "    while True:    \n",
    "        print(\"\\nMovie Recommendation Application Based On Reviews\")        \n",
    "        print(f\"Your Review: {my_review}\")\n",
    "        print(f\"Your Category: {my_category}\")\n",
    "        \n",
    "        print('1. Write your review')\n",
    "        print('2. View movie recommendation')\n",
    "        print('3. View NER')\n",
    "        print('4. Exit')\n",
    "        \n",
    "        try: \n",
    "            choice = input(\">> \")\n",
    "            if choice == '1':\n",
    "                menu_1(loaded_classifier)\n",
    "            elif choice == '2':\n",
    "                menu_2()\n",
    "            elif choice == '3':\n",
    "                menu_3()\n",
    "            elif choice == '4':\n",
    "                print(\"Exiting application...\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Invalid input. Please enter a number between 1 and 4.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a valid number.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7d53c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_menu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
